```{r}
library(RWeka)
library(partykit)
library(ROCR)
```

**PART 1**

Reading data.

```{r}
sonar = read.csv("sonar.all-data", header=FALSE)

dim(sonar)
str(sonar)
summary(sonar)

head(sonar)
```

**PART 2**

Using C4.5 decision tree.

```{r}
c45_tree_classifier <- J48(V61 ~ ., data = sonar)

c45_tree_classifier
summary(c45_tree_classifier)
table(sonar$V61, predict(c45_tree_classifier))
plot(c45_tree_classifier,
     gp = gpar(fontsize = 6),
     inner_panel=node_inner,
     ip_args=list(abbreviate = TRUE, id = FALSE)
)
```

Experimenting with different parameters.

```{r}
tree_classifier_1 <- J48(V61 ~ ., data = sonar, control = Weka_control(U = TRUE))
tree_classifier_1
summary(tree_classifier_1)

tree_classifier_2 <- J48(V61 ~ ., data = sonar, control = Weka_control(R = TRUE))
tree_classifier_2
summary(tree_classifier_2)

tree_classifier_3 <- J48(V61 ~ ., data = sonar, control = Weka_control(C = 0.1))
tree_classifier_3
summary(tree_classifier_3)

tree_classifier_4 <- J48(V61 ~ ., data = sonar, control = Weka_control(C = 0.5))
tree_classifier_4
summary(tree_classifier_4)

tree_classifier_5 <- J48(V61 ~ ., data = sonar, control = Weka_control(O = TRUE))
tree_classifier_5
summary(tree_classifier_5)

tree_classifier_6 <- J48(V61 ~ ., data = sonar, control = Weka_control(R = TRUE, N = 5))
tree_classifier_6
summary(tree_classifier_6)

tree_classifier_7 <- J48(V61 ~ ., data = sonar, control = Weka_control(R = TRUE, N = 2))
tree_classifier_7
summary(tree_classifier_7)

tree_classifier_8 <- J48(V61 ~ ., data = sonar, control = Weka_control(S = TRUE))
tree_classifier_8
summary(tree_classifier_8)

tree_classifier_9 <- J48(V61 ~ ., data = sonar, control = Weka_control(A = TRUE))
tree_classifier_9
summary(tree_classifier_9)

# Best result
tree_classifier_10 <- J48(V61 ~ ., data = sonar, control = Weka_control(J = TRUE))
tree_classifier_10
summary(tree_classifier_10)

tree_classifier_11 <- J48(V61 ~ ., data = sonar, control = Weka_control(doNotMakeSplitPointActualValue = TRUE))
tree_classifier_11
summary(tree_classifier_11)

tree_classifier_12 <- J48(V61 ~ ., data = sonar, control = Weka_control(J = TRUE, R = TRUE, N = 10))
tree_classifier_12
summary(tree_classifier_12)
```

Adding classification evaluation measures.

```{r}
findDataStats <- function(labels) {
  possible_values <- unique(labels)
  size <- length(possible_values)
  
  p_symbol <- possible_values[1]
  n_symbol <- possible_values[2]
  p <- length(which(labels == p_symbol))
  n <- length(which(labels == n_symbol))
  
  data_stats <- list(
    size = size,
    p_symbol = p_symbol,
    n_symbol = n_symbol,
    p = p,
    n = n
  )
}

findPredictionStats <- function(data_stats, labels, predictions) {
  p_symbol <- data_stats$p_symbol
  n_symbol <- data_stats$n_symbol
  
  tp <- length(which(predictions == p_symbol & labels == p_symbol))
  fp <- length(which(predictions == p_symbol & labels == n_symbol))
  tn <- length(which(predictions == n_symbol & labels == n_symbol))
  fn <- length(which(predictions == n_symbol & labels == p_symbol))
  
  prediction_stats <- list(
    tp = tp,
    fp = fp,
    tn = tn,
    fn = fn
  )
  return(prediction_stats)
}

findEvalMeasures <- function(data_stats, prediction_stats) {
  p <- data_stats$p
  n <- data_stats$n
  
  tp <- prediction_stats$tp
  tn <- prediction_stats$tn
  fp <- prediction_stats$fp
  fn <- prediction_stats$fn
  
  acc <- (tp + tn) / (p + n)
  err <- (fp + fn) / (p + n)
  rec <- tp / p
  prec <- tp / (tp + fp)
  f <- (2 * prec * rec) / (prec + rec)
  
  eval_measures <- list(
    accuracy = acc,
    error = err,
    recall = rec,
    precision = prec,
    f_score = f
  )
  return(eval_measures)
}
```

Calculating evaluation measures to C4.5 decision tree.

```{r}
predictions <- as.vector(predict(c45_tree_classifier))
labels <- as.vector(sonar$V61)

data_stats <- findDataStats(labels)
prediction_stats <- findPredictionStats(data_stats, labels, predictions)
eval_measures <- findEvalMeasures(data_stats, prediction_stats)
eval_measures
```

Using 10-fold cross validation for C4.5 decision tree.

```{r}
c45_tree_classifier_fold <- evaluate_Weka_classifier(c45_tree_classifier, numFolds = 10)

c45_tree_classifier_fold
summary(c45_tree_classifier_fold)

c45_tree_classifier_fold$details

data_stats <- findDataStats(labels)
prediction_stats <- list(
  tp = c45_tree_classifier_fold$confusionMatrix["R", "R"],
  fp = c45_tree_classifier_fold$confusionMatrix["M", "R"],
  tn = c45_tree_classifier_fold$confusionMatrix["M", "M"],
  fn = c45_tree_classifier_fold$confusionMatrix["R", "M"]
)
eval_measures <- findEvalMeasures(data_stats, prediction_stats)
eval_measures
```

It is evident that using a 10-fold validation produced worse results than using the normal decision tree but this is due to the fact that the normal decision is training and testing on the same data which results in over-fitting.

**PART 3**